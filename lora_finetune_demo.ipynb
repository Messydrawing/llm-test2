{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7129f2da",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "flowchart LR\n",
    "    A[dataset_builder] -->|原始样本| B[teacher_labeler]\n",
    "    B -->|生成标签| C[train_lora]\n",
    "    C -->|LoRA 模型| D[evaluate]\n",
    "```\n",
    "\n",
    "以上示意图展示了各模块的协作关系：\n",
    "- `dataset_builder`：收集与清洗数据，输出原始训练样本；\n",
    "- `teacher_labeler`：调用教师模型，为样本添加 `prediction`、`analysis` 等标签；\n",
    "- `train_lora`：基于带标签数据执行 LoRA 微调；\n",
    "- `evaluate`：加载微调模型，在验证集上评估效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f794a6f",
   "metadata": {},
   "source": [
    "### LoRA 原理概述\n",
    "LoRA（Low-Rank Adaptation）将权重增量表示为低秩分解 $\\Delta W = B A$，仅训练小矩阵 $A,B$，而预训练权重 $W_0$ 保持冻结不更新，从而在保持原模型能力的同时大幅减少可训练参数和显存开销。\n",
    "\n",
    "在本示例中，可选参数 `rope_factor` 会按比例缩放旋转位置编码（RoPE）的基础频率，以此扩展模型可接受的上下文长度；例如 `rope_factor=2` 时，理论上可将上下文窗口扩大到原来的两倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d23965",
   "metadata": {},
   "source": [
    "### 金融场景下的动机示例\n",
    "在金融任务中，模型需理解长篇财报、行情序列并生成投资建议。通过 LoRA 微调，我们只需在少量行业数据上训练小规模适配器，即可快速获得专用模型；配合 `rope_factor` 扩展后的 RoPE，模型还能一次性读入更长的时间区间或多份文档。这为后续的数据构建、教师标注、训练与评估提供了现实动机。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab46637",
   "metadata": {},
   "source": [
    "# 大模型 LoRA 微调流程演示\n",
    "\n",
    "本示例 Notebook 展示金融股票领域大模型微调的完整流程，包括数据集构建、教师模型标注、LoRA 微调训练以及模型评估。示例使用 `test4` 模块中的代码，并利用模拟数据和模型以便在轻量环境下演示整体流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beeef1c",
   "metadata": {},
   "source": [
    "## 背景介绍\n",
    "- **LoRA** 通过在预训练模型的部分权重上插入低秩矩阵，在冻结原模型参数的情况下完成高效微调。\n",
    "- **RoPE 长上下文扩展**: 通过设置 `rope_factor` 扩展旋转位置编码，使模型支持更长的输入序列。\n",
    "- **Token 截断**: 在构造数据集和训练时确保输入长度不超过最大上下文窗口。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be389a5b",
   "metadata": {},
   "source": [
    "## 步骤1：构建模拟数据集\n",
    "下面构造两只股票的30日K线数据，并使用 `dataset_builder.build_dataset` 生成训练/验证样本。为避免真实网络请求，我们 monkey-patch `_fetch_kline` 函数返回模拟数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd312e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json\n",
    "sys.path.append(os.getcwd())  # 使 test4 模块可导入\n",
    "\n",
    "from test4 import dataset_builder\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "dates = pd.date_range('2023-01-01', periods=30, freq='D').strftime('%Y-%m-%d')\n",
    "prices1 = 100 * np.cumprod(1 + np.random.normal(0, 0.01, size=30))\n",
    "prices2 = 50 * np.cumprod(1 + np.random.normal(0, 0.01, size=30))\n",
    "\n",
    "df_stock1 = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'open': prices1,\n",
    "    'close': prices1 + np.random.normal(0, 0.5, size=30),\n",
    "    'high': prices1 + np.random.normal(0, 1, size=30),\n",
    "    'low':  prices1 - np.random.normal(0, 1, size=30),\n",
    "    'volume': np.random.uniform(1000, 10000, size=30)\n",
    "})\n",
    "\n",
    "df_stock2 = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'open': prices2,\n",
    "    'close': prices2 + np.random.normal(0, 0.5, size=30),\n",
    "    'high': prices2 + np.random.normal(0, 1, size=30),\n",
    "    'low':  prices2 - np.random.normal(0, 1, size=30),\n",
    "    'volume': np.random.uniform(1000, 10000, size=30)\n",
    "})\n",
    "\n",
    "def dummy_fetch_kline(code: str, days: int):\n",
    "    if code == '000001':\n",
    "        return df_stock1.tail(days).reset_index(drop=True)\n",
    "    elif code == '000002':\n",
    "        return df_stock2.tail(days).reset_index(drop=True)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "dataset_builder._fetch_kline = dummy_fetch_kline\n",
    "\n",
    "train_samples, val_samples = dataset_builder.build_dataset(['000001','000002'], days=30, window=30, val_ratio=0.2, seed=42)\n",
    "print(f\"训练样本数: {len(train_samples)}, 验证样本数: {len(val_samples)}\")\n",
    "if train_samples:\n",
    "    example_prompt = dataset_builder.format_prompt(train_samples[0])\n",
    "    print(example_prompt[:120] + '...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d29abfb",
   "metadata": {},
   "source": [
    "## 步骤2：教师模型标注\n",
    "使用 `teacher_labeler` 将 Prompt 列表转换为带有 `prediction`、`analysis` 和 `advice` 字段的答案。这里的 `call_teacher` 函数被替换为返回固定内容的模拟教师模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c388d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test4 import teacher_labeler\n",
    "\n",
    "def dummy_call_teacher(prompt: str):\n",
    "    if '000001' in prompt:\n",
    "        pred = '上涨'\n",
    "    else:\n",
    "        pred = '下跌'\n",
    "    return json.dumps({'prediction': pred, 'analysis': '模拟分析', 'advice': '模拟建议'}, ensure_ascii=False)\n",
    "\n",
    "teacher_labeler.call_teacher = dummy_call_teacher\n",
    "\n",
    "train_prompts = [dataset_builder.format_prompt(s) for s in train_samples]\n",
    "val_prompts = [dataset_builder.format_prompt(s) for s in val_samples]\n",
    "\n",
    "train_records = teacher_labeler.label_samples(train_prompts, output_file='labeled_data.jsonl')\n",
    "val_records = teacher_labeler.label_samples(val_prompts, output_file='val_labeled_data.jsonl')\n",
    "\n",
    "print(f\"已标注训练样本数: {len(train_records)}, 验证样本数: {len(val_records)}\")\n",
    "print(json.dumps(train_records[0], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e416f90",
   "metadata": {},
   "source": [
    "## 步骤3：LoRA 微调训练\n",
    "下面调用 `train_lora.main` 进行一次简化训练。为了便于快速运行，使用体积很小的 `sshleifer/tiny-gpt2` 作为基础模型，并将训练步数限制为 1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d46c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from test4 import train_lora\n",
    "\n",
    "if not hasattr(train_lora, 'orig_BitsAndBytesConfig'):\n",
    "    train_lora.orig_BitsAndBytesConfig = train_lora.BitsAndBytesConfig\n",
    "\n",
    "def safe_bnb_config(**kwargs):\n",
    "    try:\n",
    "        import torch\n",
    "        if torch.cuda.is_available():\n",
    "            return train_lora.orig_BitsAndBytesConfig(**kwargs)\n",
    "    except ImportError:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "train_lora.BitsAndBytesConfig = safe_bnb_config\n",
    "\n",
    "cfg = train_lora.TrainConfig(\n",
    "    base_model='sshleifer/tiny-gpt2',\n",
    "    data_path='labeled_data.jsonl',\n",
    "    eval_path='val_labeled_data.jsonl',\n",
    "    output_dir='lora_demo',\n",
    "    epochs=1,\n",
    "    max_steps=1,\n",
    "    batch_size=1,\n",
    "    max_len=4096,\n",
    "    rope_factor=2.0\n",
    ")\n",
    "\n",
    "train_lora.main(cfg)\n",
    "print('LoRA 微调完成')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6154fa0",
   "metadata": {},
   "source": [
    "## 步骤4：模型评估\n",
    "示例中我们基于验证集计算 BLEU 分数。为了展示流程，假设基座模型输出为空 JSON，而微调模型能够准确复现教师答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a2356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "prompts = [rec['prompt'] for rec in val_records]\n",
    "refs = [json.dumps(rec['label'], ensure_ascii=False, sort_keys=True) for rec in val_records]\n",
    "\n",
    "base_preds = ['{}' for _ in prompts]\n",
    "# 模拟微调模型输出等同于参考答案\n",
    "with open('val_labeled_data.jsonl', encoding='utf-8') as f:\n",
    "    tuned_preds = [json.loads(line)['label'] for line in f]\n",
    "    tuned_preds = [json.dumps(t, ensure_ascii=False, sort_keys=True) for t in tuned_preds]\n",
    "\n",
    "smooth = SmoothingFunction().method4\n",
    "base_bleu = sum(sentence_bleu([r.split()], p.split(), smoothing_function=smooth) for r,p in zip(refs, base_preds)) / len(refs)\n",
    "tuned_bleu = sum(sentence_bleu([r.split()], p.split(), smoothing_function=smooth) for r,p in zip(refs, tuned_preds)) / len(refs)\n",
    "print(f\"BLEU 未微调: {base_bleu:.4f}, 微调后: {tuned_bleu:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae2af3f",
   "metadata": {},
   "source": [
    "## 总结\n",
    "通过以上步骤，我们演示了基于 `test4` 模块的完整 LoRA 微调流程：从构建股票K线数据集、教师模型标注，到对学生模型进行 LoRA 微调并进行指标评估。该流程能够在有限资源下让模型学习金融领域知识。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
