{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA 流水线（Test4）\n",
    "\n",
    "按顺序运行各部分以复现整个流水线。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 安装依赖（如有需要运行一次）\n",
    "# !pip -q install transformers==4.53.0 datasets==3.6.0 peft==0.15.2 #     accelerate>=0.33.0 torch==2.7.1 numpy==2.3.1 pandas==2.2.2 #     evaluate==0.4.0 nltk rouge-score sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 配置\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\nSTOCK_CODES = [\n    \"600000\",\n    \"002142\",\n    \"600036\",\n    \"600926\",\n    \"000001\",\n    \"601398\",\n    \"601998\",\n    \"601328\",\n]\nSUMMARY_DAYS = 90"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据获取\n",
    "\n",
    "`EastMoneyAPI` 用于下载所设定股票的 K 线数据。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "class EastMoneyAPI:\n",
    "    \"\"\"用于获取东财 K 线数据的简单封装。\"\"\"\n",
    "    KLINE_URL = \"https://push2his.eastmoney.com/api/qt/stock/kline/get\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.session = requests.Session()\n",
    "\n",
    "    def _secid(self, code: str) -> str:\n",
    "        return f\"{'1' if code.startswith('6') else '0'}.{code}\"\n",
    "\n",
    "    def get_kline_data(self, stock_code: str, klt: int = 101, num: int = 1000) -> pd.DataFrame | None:\n",
    "        params = {\n",
    "            \"secid\": self._secid(stock_code),\n",
    "            \"klt\": klt,\n",
    "            \"fqt\": 0,\n",
    "            \"lmt\": num,\n",
    "            \"end\": \"20500000\",\n",
    "            \"beg\": \"0\",\n",
    "            \"fields1\": \"f1,f2,f3,f4,f5,f6\",\n",
    "            \"fields2\": \"f51,f52,f53,f54,f55,f56,f57,f58\",\n",
    "        }\n",
    "        try:\n",
    "            r = self.session.get(self.KLINE_URL, params=params, timeout=8)\n",
    "            r.raise_for_status()\n",
    "            js = r.json()\n",
    "            klines = js[\"data\"][\"klines\"]\n",
    "            records = []\n",
    "            for line in klines:\n",
    "                d = line.split(\",\")\n",
    "                records.append(\n",
    "                    {\n",
    "                        \"date\": d[0],\n",
    "                        \"open\": float(d[1]),\n",
    "                        \"close\": float(d[2]),\n",
    "                        \"high\": float(d[3]),\n",
    "                        \"low\": float(d[4]),\n",
    "                        \"volume\": float(d[5]),\n",
    "                    }\n",
    "                )\n",
    "            df = pd.DataFrame(records)\n",
    "            df.sort_values(\"date\", inplace=True)\n",
    "            df.reset_index(drop=True, inplace=True)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"[EastMoneyAPI] {stock_code} 获取失败: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "def get_recent_data(days: int = SUMMARY_DAYS) -> dict[str, pd.DataFrame]:\n",
    "    api = EastMoneyAPI()\n",
    "    today = datetime.date.today()\n",
    "    start_date = today - datetime.timedelta(days=days + 10)\n",
    "    stock_data = {}\n",
    "    for code in STOCK_CODES:\n",
    "        df = api.get_kline_data(code, num=1000)\n",
    "        if df is not None:\n",
    "            df = df[df[\"date\"] >= start_date.strftime(\"%Y-%m-%d\")]\n",
    "            stock_data[code] = df\n",
    "    return stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 数据集构建\n",
    "\n",
    "根据 K 线数据生成滑动窗口样本，并为教师模型格式化提示。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\nimport json, random\nfrom typing import Any, Sequence, Tuple, Optional\n\n\ndef _fetch_kline(code: str, days: int):\n    api = EastMoneyAPI()\n    df = api.get_kline_data(code, num=days)\n    if df is None:\n        return None\n    return df.tail(days).reset_index(drop=True)\n\n\ndef _sample_windows(df, window: int, num: int, rng: random.Random):\n    if len(df) < window:\n        return []\n    return [df.iloc[i : i + window].reset_index(drop=True) for i in range(len(df) - window + 1)]\n\n\ndef _make_prompt(window) -> dict[str, Any]:\n    change = ((window[\"close\"].iloc[-1] / window[\"close\"].iloc[0]) - 1) * 100\n    summary = window.to_dict(orient=\"records\")\n    return {\n        \"change\": round(change, 2),\n        \"prediction\": \"\",\n        \"analysis\": \"\",\n        \"advice\": \"\",\n        \"kline_summary\": summary,\n    }\n\n\ndef format_prompt(sample: dict[str, Any]) -> str:\n    summary = json.dumps(sample[\"kline_summary\"], ensure_ascii=False)\n    return (\n        f\"股票 {sample['stock_code']} 近30日K线数据: {summary}\n\"\n        f\"涨跌幅: {sample['change']}%。请预测后市走势，\"\n        \"给出简短分析和操作建议，\"\n        \"并以 JSON 格式回复，包括 'prediction', 'analysis', 'advice' 三个字段。\"\n    )\n\n\ndef _trim_sample_tokens(sample: dict[str, Any], tokenizer, max_tokens: int) -> None:\n    if tokenizer is None:\n        return\n    while len(sample[\"kline_summary\"]) > 1:\n        text = format_prompt(sample)\n        if len(tokenizer(text, add_special_tokens=False)[\"input_ids\"]) <= max_tokens:\n            break\n        sample[\"kline_summary\"].pop(0)\n\n\ndef build_dataset(\n    stock_codes: Sequence[str],\n    *,\n    days: int = 180,\n    window: int = 30,\n    windows_per_stock: int = 1,\n    val_ratio: float = 0.2,\n    seed: int | None = None,\n    tokenizer=None,\n    max_tokens: int = 1024,\n    balance_classes: bool = True,\n) -> Tuple[list[dict[str, Any]], list[dict[str, Any]]]:\n    codes = list(stock_codes)\n    rng = random.Random(seed)\n    up_samples: list[dict[str, Any]] = []\n    down_samples: list[dict[str, Any]] = []\n    stable_samples: list[dict[str, Any]] = []\n\n    import pandas as pd\n    import numpy as np\n\n    for code in codes:\n        df = _fetch_kline(code, days)\n        if df is None or df.empty:\n            continue\n        df[\"pct_chg\"] = df[\"close\"].pct_change() * 100\n        df[\"pct_chg\"] = df[\"pct_chg\"].fillna(0)\n        df[\"MA5\"] = df[\"close\"].rolling(5).mean()\n        df[\"MA10\"] = df[\"close\"].rolling(10).mean()\n        differences = df[\"close\"].diff()\n        gains = differences.clip(lower=0)\n        losses = -differences.clip(upper=0)\n        avg_gain = gains.ewm(alpha=1 / 14, adjust=False).mean()\n        avg_loss = losses.ewm(alpha=1 / 14, adjust=False).mean()\n        rs = avg_gain / avg_loss\n        rs_values = rs.to_numpy()\n        avg_gain_values = avg_gain.to_numpy()\n        avg_loss_values = avg_loss.to_numpy()\n        rsi = np.where(\n            avg_loss_values == 0,\n            np.where(avg_gain_values == 0, 50, 100),\n            100 - 100 / (1 + rs_values),\n        )\n        df[\"RSI14\"] = rsi\n        ema12 = df[\"close\"].ewm(span=12, adjust=False).mean()\n        ema26 = df[\"close\"].ewm(span=26, adjust=False).mean()\n        df[\"MACD\"] = ema12 - ema26\n        df[\"MA5\"] = df[\"MA5\"].round(2)\n        df[\"MA10\"] = df[\"MA10\"].round(2)\n        df[\"RSI14\"] = df[\"RSI14\"].round(2)\n        df[\"MACD\"] = df[\"MACD\"].round(2)\n        n = len(df)\n        if n < window:\n            continue\n        windows_cat: list[tuple[str, dict[str, Any]]] = []\n        for i in range(n - window + 1):\n            if (\n                df[\"volume\"].iloc[i : i + window].eq(0).any()\n                or df[\"pct_chg\"].iloc[i : i + window].abs().gt(20).any()\n            ):\n                continue\n            win = df.iloc[i : i + window][\n                [\n                    \"date\",\n                    \"open\",\n                    \"close\",\n                    \"high\",\n                    \"low\",\n                    \"volume\",\n                    \"MA5\",\n                    \"MA10\",\n                    \"RSI14\",\n                    \"MACD\",\n                ]\n            ].reset_index(drop=True)\n            change_percent = ((win[\"close\"].iloc[-1] / win[\"close\"].iloc[0]) - 1) * 100\n            if change_percent > 3:\n                category = \"up\"\n            elif change_percent < -3:\n                category = \"down\"\n            else:\n                category = \"stable\"\n            prompt = _make_prompt(win)\n            prompt[\"stock_code\"] = code\n            windows_cat.append((category, prompt))\n\n        rng.shuffle(windows_cat)\n        for category, prompt in windows_cat[:windows_per_stock]:\n            if tokenizer:\n                text = format_prompt(prompt)\n                while (\n                    len(tokenizer(text, add_special_tokens=False)[\"input_ids\"]) > max_tokens\n                    and prompt[\"kline_summary\"]\n                ):\n                    prompt[\"kline_summary\"].pop(0)\n                    text = format_prompt(prompt)\n            if category == \"up\":\n                up_samples.append(prompt)\n            elif category == \"down\":\n                down_samples.append(prompt)\n            else:\n                stable_samples.append(prompt)\n    if balance_classes and up_samples and down_samples and stable_samples:\n        min_count = min(len(up_samples), len(down_samples), len(stable_samples))\n        rng.shuffle(up_samples)\n        rng.shuffle(down_samples)\n        rng.shuffle(stable_samples)\n        up_samples = up_samples[:min_count]\n        down_samples = down_samples[:min_count]\n        stable_samples = stable_samples[:min_count]\n    samples = up_samples + down_samples + stable_samples\n    rng.shuffle(samples)\n    split = int(len(samples) * (1 - val_ratio))\n    return samples[:split], samples[split:]"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 教师标注\n",
    "\n",
    "将提示发送到远程教师模型（DeepSeek-R1）。如果未设置 `ARK_API_KEY`，调用将返回占位符。清洗后的标签会与提示一起保存。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "ARK_API_KEY = os.getenv(\"ARK_API_KEY\", \"\")\n",
    "\n",
    "\n",
    "def call_teacher(prompt: str) -> dict[str, str]:\n",
    "    if not ARK_API_KEY:\n",
    "        return {\"content\": \"[缺少 ARK_API_KEY]\", \"reasoning\": \"\"}\n",
    "    try:\n",
    "        from volcenginesdkarkruntime import Ark\n",
    "        client = Ark(api_key=ARK_API_KEY)\n",
    "        resp = client.chat.completions.create(\n",
    "            model=\"deepseek-r1-250528\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        )\n",
    "        msg = resp.choices[0].message\n",
    "        content = msg.content.strip()\n",
    "        reasoning = getattr(msg, \"reasoning_content\", \"\").strip()\n",
    "        return {\"content\": content, \"reasoning\": reasoning}\n",
    "    except Exception as e:\n",
    "        return {\"content\": f\"[教师模型错误: {e}]\", \"reasoning\": \"\"}\n",
    "\n",
    "\n",
    "def label_samples(prompts, output_file=\"labeled_data.jsonl\"):\n",
    "    path = Path(output_file)\n",
    "    labeled = []\n",
    "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        for prompt in prompts:\n",
    "            ans = call_teacher(prompt)\n",
    "            answer = ans.get(\"content\", ans) if isinstance(ans, dict) else ans\n",
    "            try:\n",
    "                label = json.loads(answer)\n",
    "            except (TypeError, json.JSONDecodeError):\n",
    "                label = {\"raw\": answer}\n",
    "            if not isinstance(label, dict):\n",
    "                label = {\"raw\": str(label)}\n",
    "            label[\"prediction\"] = str(label.get(\"prediction\", \"\"))\n",
    "            label[\"analysis\"] = str(label.get(\"analysis\", \"\"))\n",
    "            label[\"advice\"] = str(label.get(\"advice\", \"\"))\n",
    "            if isinstance(ans, dict) and ans.get(\"reasoning\"):\n",
    "                label[\"reasoning\"] = str(ans[\"reasoning\"])\n",
    "            record = {\"prompt\": prompt, \"label\": label}\n",
    "            labeled.append(record)\n",
    "            f.write(json.dumps(record, ensure_ascii=False) + \"\n",
    "\")\n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 数据清洗\n",
    "\n",
    "移除无效记录并确保标签为 JSON 字符串。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "\n",
    "def normalize_label(lab):\n",
    "    if isinstance(lab, str) and lab.strip():\n",
    "        return lab.strip()\n",
    "    if isinstance(lab, dict):\n",
    "        lab[\"prediction\"] = str(lab.get(\"prediction\", \"\"))\n",
    "        lab[\"analysis\"] = str(lab.get(\"analysis\", \"\"))\n",
    "        lab[\"advice\"] = str(lab.get(\"advice\", \"\"))\n",
    "        if any(lab[k] for k in (\"prediction\", \"analysis\", \"advice\")):\n",
    "            return json.dumps(lab, ensure_ascii=False, sort_keys=True)\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_jsonl(inp: str, out: str) -> None:\n",
    "    stats = Counter()\n",
    "    good = []\n",
    "    for raw in Path(inp).read_text(encoding=\"utf-8\").splitlines():\n",
    "        if not raw.strip():\n",
    "            stats[\"empty_line\"] += 1\n",
    "            continue\n",
    "        try:\n",
    "            rec = json.loads(raw)\n",
    "        except json.JSONDecodeError:\n",
    "            stats[\"invalid_json\"] += 1\n",
    "            continue\n",
    "        prompt = rec.get(\"prompt\", \"\").strip()\n",
    "        label = normalize_label(rec.get(\"label\"))\n",
    "        if prompt and label:\n",
    "            good.append(json.dumps({\"prompt\": prompt, \"label\": label}, ensure_ascii=False))\n",
    "            stats[\"kept\"] += 1\n",
    "        else:\n",
    "            stats[\"bad_schema\"] += 1\n",
    "    Path(out).write_text(\"\n",
    "\".join(good) + (\"\n",
    "\" if good else \"\"), \"utf-8\")\n",
    "    print(f\"[clean_jsonl] 写入 {stats['kept']} 条有效样本 → {out}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LoRA 微调\n",
    "\n",
    "在已标注的数据集上使用 LoRA 适配器微调基础模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\nimport inspect\nfrom dataclasses import dataclass\nimport torch\nimport torch.optim.lr_scheduler as lr_sched\nfrom datasets import Dataset\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    Trainer,\n)\n\nif not hasattr(lr_sched, \"LRScheduler\"):\n    lr_sched.LRScheduler = lr_sched._LRScheduler\n\n@dataclass\nclass TrainConfig:\n    base_model: str = \"Qwen/Qwen1.5-7B\"\n    data_path: str = \"labeled_data.jsonl\"\n    eval_path: str | None = None\n    output_dir: str = \"lora_adapter\"\n    batch_size: int = 1\n    lr: float = 2e-4\n    epochs: int | None = 1\n    max_steps: int | None = None\n    grad_accum: int = 4\n    max_len: int = 4096\n    rope_factor: float = 1.0\n\nIGNORE_INDEX = -100\n\ndef _load_dataset(path: str) -> Dataset:\n    recs = []\n    with open(path, encoding=\"utf-8\") as f:\n        for line in f:\n            rec = json.loads(line)\n            prompt = rec[\"prompt\"].strip()\n            label = rec[\"label\"]\n            if isinstance(label, dict):\n                label = json.dumps(label, ensure_ascii=False, sort_keys=True)\n            else:\n                label = str(label).strip()\n            recs.append({\"prompt\": prompt, \"label\": label})\n    return Dataset.from_list(recs)\n\nclass LabelCollator:\n    def __init__(self, tokenizer, max_len: int = 1024):\n        self.tok = tokenizer\n        self.max_len = max_len\n        if self.tok.pad_token is None:\n            self.tok.pad_token = self.tok.eos_token\n\n    def __call__(self, batch):\n        texts, prompt_lens = [], []\n        for rec in batch:\n            prompt = rec[\"prompt\"].strip()\n            label = rec[\"label\"].strip()\n            full = f\"{prompt}\n\n### 答案：{label}\"\n            plen = len(self.tok(prompt + \"\n\n### 答案：\")[\"input_ids\"])\n            prompt_lens.append(plen)\n            texts.append(full)\n        enc = self.tok(texts, padding=\"longest\", truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n        labels = enc[\"input_ids\"].clone()\n        labels[:, :] = IGNORE_INDEX\n        for i, plen in enumerate(prompt_lens):\n            labels[i, plen : enc[\"input_ids\"].size(1)] = enc[\"input_ids\"][i, plen:]\n        if (labels != IGNORE_INDEX).sum() == 0:\n            raise ValueError(\"全部 label 被截掉；请缩短 prompt 或增大 --max-len\")\n        enc[\"labels\"] = labels\n        return enc\n\ndef train_lora(cfg: TrainConfig) -> None:\n    train_ds = _load_dataset(cfg.data_path)\n    eval_ds = _load_dataset(cfg.eval_path) if cfg.eval_path else None\n    tokenizer = AutoTokenizer.from_pretrained(cfg.base_model, trust_remote_code=True)\n    bnb = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_use_double_quant=True,\n        bnb_4bit_compute_dtype=torch.float16,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        cfg.base_model,\n        device_map=\"auto\",\n        quantization_config=bnb,\n        trust_remote_code=True,\n    )\n    model.config.use_cache = False\n    if cfg.rope_factor and cfg.rope_factor != 1.0:\n        model.config.rope_scaling = {\"type\": \"linear\", \"factor\": cfg.rope_factor}\n        base_pos = getattr(model.config, \"max_position_embeddings\", 2048)\n        model.config.max_position_embeddings = int(base_pos * cfg.rope_factor)\n    lora_cfg = LoraConfig(\n        r=8,\n        lora_alpha=32,\n        lora_dropout=0.05,\n        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n    )\n    args_kwargs = dict(\n        output_dir=cfg.output_dir,\n        per_device_train_batch_size=cfg.batch_size,\n        gradient_accumulation_steps=cfg.grad_accum,\n        num_train_epochs=cfg.epochs or 1,\n        max_steps=cfg.max_steps or -1,\n        learning_rate=cfg.lr,\n        logging_steps=1,\n        remove_unused_columns=False,\n    )\n    sig = inspect.signature(TrainingArguments.__init__)\n    if \"evaluation_strategy\" in sig.parameters:\n        args_kwargs[\"evaluation_strategy\"] = \"epoch\" if eval_ds else \"no\"\n    if \"save_strategy\" in sig.parameters:\n        args_kwargs[\"save_strategy\"] = \"epoch\"\n    args = TrainingArguments(**args_kwargs)\n    model = prepare_model_for_kbit_training(model)\n    model = get_peft_model(model, lora_cfg)\n    collator = LabelCollator(tokenizer, cfg.max_len)\n    trainer = Trainer(model=model, args=args, train_dataset=train_ds, eval_dataset=eval_ds, data_collator=collator)\n    trainer.train()\n    model.save_pretrained(cfg.output_dir)\n    tokenizer.save_pretrained(cfg.output_dir)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 评估\n",
    "\n",
    "计算参考答案与模型预测之间的 BLEU、ROUGE-L 和向量嵌入相似度。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "\nfrom nltk.translate.bleu_score import SmoothingFunction, sentence_bleu\nfrom rouge_score import rouge_scorer\nfrom sentence_transformers import SentenceTransformer, util\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n\ndef load_dataset(path: str) -> tuple[list[str], list[str]]:\n    prompts, refs = [], []\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            rec = json.loads(line)\n            prompts.append(rec[\"prompt\"])\n            label = rec.get(\"label\", \"\")\n            if isinstance(label, dict):\n                refs.append(json.dumps(label, ensure_ascii=False, sort_keys=True))\n            else:\n                refs.append(str(label))\n    return prompts, refs\n\n\ndef load_student(model_name: str):\n    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", trust_remote_code=True)\n    return tokenizer, model.eval()\n\n\ndef call_student(tokenizer, model, prompt: str, max_new_tokens: int = 256, max_length: int = 4096) -> str:\n    prompt = prompt.rstrip() + \"\n\n### 答案：\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    out = model.generate(\n        **inputs,\n        max_new_tokens=max_new_tokens,\n        max_length=max_length,\n        do_sample=False,\n        pad_token_id=tokenizer.eos_token_id,\n        use_cache=True,\n    )\n    new_tokens = out[0][inputs[\"input_ids\"].size(1) :]\n    return tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n\n\ndef generate_predictions(model_name: str, prompts):\n    tokenizer, model = load_student(model_name)\n    preds = []\n    for p in prompts:\n        preds.append(call_student(tokenizer, model, p))\n    return preds\n\n\ndef bleu_score(references, predictions):\n    smooth = SmoothingFunction().method4\n    scores = []\n    for ref, pred in zip(references, predictions):\n        scores.append(sentence_bleu([ref.split()], pred.split(), smoothing_function=smooth))\n    return sum(scores) / len(scores)\n\n\ndef rouge_l(references, predictions):\n    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n    scores = []\n    for ref, pred in zip(references, predictions):\n        result = scorer.score(ref, pred)\n        scores.append(result[\"rougeL\"].fmeasure)\n    return sum(scores) / len(scores)\n\n\ndef embedding_similarity(references, predictions):\n    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n    sims = []\n    for ref, pred in zip(references, predictions):\n        ref_emb = model.encode(ref, convert_to_tensor=True)\n        pred_emb = model.encode(pred, convert_to_tensor=True)\n        sims.append(util.cos_sim(ref_emb, pred_emb).item())\n    return sum(sims) / len(sims)\n\n\ndef evaluate_model(model_name: str, prompts, refs):\n    preds = generate_predictions(model_name, prompts)\n    return {\n        \"bleu\": bleu_score(refs, preds),\n        \"rougeL\": rouge_l(refs, preds),\n        \"embed\": embedding_similarity(refs, preds),\n    }"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 运行流水线\n",
    "\n",
    "以下单元将所有步骤串联：\n",
    "1. 构建训练集和验证集。\n",
    "2. 使用教师模型进行标注（如果提供 API key）。\n",
    "3. 清理 JSONL 文件。\n",
    "4. 使用 LoRA 微调学生模型。\n",
    "5. 在验证集上进行评估。\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# 参数\n",
    "windows = 1          # 每只股票的样本数\n",
    "val_ratio = 0.2      # 验证集比例\n",
    "max_tokens = 1024    # 提示的 token 上限\n",
    "max_len = 1024       # 训练序列长度\n",
    "output_dir = \"lora_adapter\"  # 保存适配器的位置\n",
    "\n",
    "# 1) 构建数据集\n",
    "train_set, val_set = build_dataset(STOCK_CODES, days=180, window=30,\n",
    "                                   windows_per_stock=windows, val_ratio=val_ratio,\n",
    "                                   tokenizer=None, max_tokens=max_tokens)\n",
    "train_prompts = [format_prompt(s) for s in train_set]\n",
    "val_prompts = [format_prompt(s) for s in val_set]\n",
    "\n",
    "# 2) 教师模型标注\n",
    "label_samples(train_prompts, \"labeled_data.jsonl\")\n",
    "if val_prompts:\n",
    "    label_samples(val_prompts, \"val_labeled_data.jsonl\")\n",
    "\n",
    "# 3) 清理 JSONL\n",
    "clean_jsonl(\"labeled_data.jsonl\", \"cleaned_labeled_data.jsonl\")\n",
    "if val_prompts:\n",
    "    clean_jsonl(\"val_labeled_data.jsonl\", \"cleaned_val_labeled_data.jsonl\")\n",
    "\n",
    "# 4) LoRA 训练\n",
    "cfg = TrainConfig(data_path=\"cleaned_labeled_data.jsonl\",\n",
    "                  eval_path=\"cleaned_val_labeled_data.jsonl\" if val_prompts else None,\n",
    "                  output_dir=output_dir,\n",
    "                  max_steps=200,\n",
    "                  max_len=max_len)\n",
    "train_lora(cfg)\n",
    "\n",
    "# 5) 评估\n",
    "prompts, refs = load_dataset(\"cleaned_val_labeled_data.jsonl\" if val_prompts else \"cleaned_labeled_data.jsonl\")\n",
    "metrics = evaluate_model(output_dir, prompts, refs)\n",
    "print(\"验证指标:\n",
    "\", json.dumps(metrics, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
